{
  "name": "unified-llm-api-gateway",
  "version": "1.0.0",
  "description": "Unified LLM API Gateway with multi-provider support, load balancing, retry, and rate limiting",
  "main": "dist/index.js",
  "scripts": {
    "dev": "vercel dev",
    "build": "tsc",
    "start": "node dist/index.js",
    "type-check": "tsc --noEmit"
  },
  "keywords": [
    "llm",
    "api",
    "gateway",
    "openai",
    "anthropic",
    "gemini",
    "load-balancing"
  ],
  "license": "MIT",
  "dependencies": {
    "@anthropic-ai/sdk": "^0.27.0",
    "@google/generative-ai": "^0.19.0",
    "@aws-sdk/client-bedrock-runtime": "^3.637.0",
    "axios": "^1.7.7",
    "openai": "^4.58.1"
  },
  "devDependencies": {
    "@types/node": "^20.10.0",
    "@vercel/node": "^3.0.21",
    "typescript": "^5.3.3"
  }
}
